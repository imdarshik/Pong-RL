{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "from assignment3_utils import process_frame\n",
    "import warnings\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Suppress Warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DQN Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        \"\"\"\n",
    "        Initialize the DQN model.\n",
    "\n",
    "        Parameters:\n",
    "        - input_shape: Tuple representing the shape of the input (channels:4, height:84, width:80).\n",
    "        - num_actions: Number of possible actions the agent can take.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(self._calculate_conv_output_size(input_shape), 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def _calculate_conv_output_size(self, input_shape):\n",
    "        \"\"\"\n",
    "        Calculate the output size of the convolutional layers.\n",
    "\n",
    "        Parameters:\n",
    "        - input_shape: Tuple representing the shape of the input.\n",
    "\n",
    "        Returns:\n",
    "        - The size of the output after the convolutional layers.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.conv3(self.conv2(self.conv1(torch.zeros(1, *input_shape)))).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        # Pass input through convolutional layers with ReLU activation\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Replay memory class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceMemory:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initialize the replay memory.\n",
    "\n",
    "        Parameters:\n",
    "        - capacity: Maximum number of experiences to store in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        Add a new experience to the buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - experience: A tuple containing (state, action, reward, done, next_state).\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_size: Number of experiences to sample.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple of arrays: (states, actions, rewards, dones, next_states).\n",
    "        \"\"\"\n",
    "        batch_size = min(batch_size, len(self.buffer))  # Ensure batch size is not larger than buffer size\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)  # Randomly select indices\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])  # Extract experiences\n",
    "        \n",
    "        # Convert to numpy arrays for easier manipulation\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.uint8),\n",
    "            np.array(next_states, dtype=np.float32)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Agent class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, start_epsilon, batch_size, gamma, root_dir, train_mode=False):\n",
    "        \"\"\"\n",
    "        Initialize the Agent.\n",
    "\n",
    "        Parameters:\n",
    "        - env: The environment object.\n",
    "        - start_epsilon: Initial epsilon value for epsilon-greedy policy.\n",
    "        - batch_size: Number of samples per batch for training.\n",
    "        - gamma: Discount factor for future rewards.\n",
    "        - root_dir: path to model directory to save checkpoint\n",
    "        - train_mode: Boolean indicating whether the agent is in training mode.\n",
    "        \"\"\"\n",
    "        # Set device for PyTorch\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = start_epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.root_dir = root_dir\n",
    "        self.replay_memory = ExperienceMemory(10000)\n",
    "        self.model = DQN((4, 84, 80), env.action_space.n).to(self.device)\n",
    "        self.target_model = DQN((4, 84, 80), env.action_space.n).to(self.device)\n",
    "        self.train_mode = train_mode\n",
    "        self.episode = 0\n",
    "        self.learns = 0\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "        self.reset()\n",
    "        self.replay = namedtuple('Replay', field_names=['state', 'action', 'reward', 'done', 'next_state']) # Define named tuple for replay memory\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and initialize state, steps, and total reward.\n",
    "        \"\"\"\n",
    "        self.state = process_frame(self.env.reset(seed=0)[0])\n",
    "        self.steps = 0\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def act(self):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state.\n",
    "\n",
    "        Returns:\n",
    "        - action: The chosen action.\n",
    "        \"\"\"\n",
    "        if self.train_mode and np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state_tensor = torch.tensor(np.array(self.state), dtype=torch.float32).to(self.device)\n",
    "        return np.argmax(self.model(state_tensor).cpu().detach().numpy())\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Perform one step of interaction with the environment and store the experience.\n",
    "\n",
    "        Returns:\n",
    "        - done: Boolean indicating if the episode is finished.\n",
    "        - episode_reward: Total reward for the episode if done, else None.\n",
    "        \"\"\"\n",
    "        action = self.act()\n",
    "        next_state, reward, done, _, _ = self.env.step(action)\n",
    "        next_state = process_frame(next_state)\n",
    "        self.replay_memory.append(self.replay(np.squeeze(self.state, axis=0), action, reward, done, np.squeeze(next_state, axis=0)))\n",
    "        self.state = next_state\n",
    "        self.steps += 1\n",
    "        self.total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            episode_reward = self.total_reward\n",
    "            print(f\"Steps: {self.steps}, Score: {episode_reward}\")\n",
    "            self.episode += 1\n",
    "            self.reset()\n",
    "            return True, episode_reward\n",
    "\n",
    "        return False, None\n",
    "\n",
    "    def update_weights(self):\n",
    "        \"\"\"\n",
    "        Update the weights of the model based on experiences sampled from the replay memory.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, dones, next_states = self.replay_memory.sample(self.batch_size)\n",
    "\n",
    "        states_t = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        next_states_t = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        actions_t = torch.tensor(actions).to(self.device)\n",
    "        rewards_t = torch.tensor(rewards).to(self.device)\n",
    "        done_mask = torch.BoolTensor(dones).to(self.device)\n",
    "\n",
    "        current_q_values = self.model(states_t).gather(1, actions_t.unsqueeze(-1)).squeeze(-1)\n",
    "        next_q_values = self.target_model(next_states_t).max(1)[0]\n",
    "        next_q_values[done_mask] = 0.0\n",
    "        next_q_values = next_q_values.detach()\n",
    "\n",
    "        expected_q_values = rewards_t + self.gamma * next_q_values\n",
    "        loss = F.mse_loss(current_q_values, expected_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.learns += 1\n",
    "\n",
    "        if self.learns % 1000 == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "            print(f\"Episode {self.episode}: Target model weights updated\")\n",
    "\n",
    "    def demo(self, model_path):\n",
    "        \"\"\"\n",
    "        Demonstrate the agent's performance using a pre-trained model.\n",
    "\n",
    "        Parameters:\n",
    "        - model_path: Path to the pre-trained model.\n",
    "        \"\"\"\n",
    "        self.load(model_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.act()\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            self.state = process_frame(next_state)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        print(f\"Steps: {steps}, Reward: {episode_reward:.2f}\")\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the current model's state dictionary to a file.\n",
    "\n",
    "        Parameters:\n",
    "        - path: Path to save the model.\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load a model's state dictionary from a file.\n",
    "\n",
    "        Parameters:\n",
    "        - path: Path to load the model from.\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def save_checkpoint(self, episode):\n",
    "        \"\"\"\n",
    "        Save a checkpoint of the current training state.\n",
    "\n",
    "        Parameters:\n",
    "        - episode: Current episode number.\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            'episode': episode,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }\n",
    "        torch.save(checkpoint, '{}/checkpoint.pth'.format(self.root_dir))\n",
    "        print(f'Checkpoint saved at episode {episode}')\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        \"\"\"\n",
    "        Load a checkpoint of the training state.\n",
    "\n",
    "        Parameters:\n",
    "        - filename: Path to the checkpoint file.\n",
    "\n",
    "        Returns:\n",
    "        - episode: Episode number from the checkpoint.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(filename)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        return checkpoint['episode']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(params):\n",
    "    \"\"\"\n",
    "    Train the agent using the provided parameters.\n",
    "    \n",
    "    Args:\n",
    "    params (dict): A dictionary containing training parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Environment setup\n",
    "    env = FrameStack(gym.make('PongDeterministic-v4', render_mode='rgb_array'), num_stack=4)\n",
    "\n",
    "    # Training setup\n",
    "    writer = SummaryWriter()\n",
    "    agent = Agent(env, \n",
    "                  params[\"EPSILON_INIT\"],\n",
    "                  params[\"BATCH_SIZE\"],\n",
    "                  params[\"GAMMA\"], \n",
    "                  params[\"ROOT_DIR\"],\n",
    "                  params[\"TRAINING_MODE\"])\n",
    "    episode_rewards = []\n",
    "    ave_rewards = []\n",
    "\n",
    "    for episode in range(1, params[\"EPISODES\"]):\n",
    "        terminate = False\n",
    "        while not terminate:\n",
    "            # Decay epsilon for exploration-exploitation trade-off\n",
    "            agent.epsilon = max(agent.epsilon * params[\"EPSILON_DECAY\"], params[\"EPSILON_MIN\"])\n",
    "            \n",
    "            # Perform one step of training\n",
    "            terminate, reward = agent.train()\n",
    "            \n",
    "            if terminate:\n",
    "                # Record and log episode results\n",
    "                episode_rewards.append(reward)\n",
    "                mean_reward = round(np.mean(episode_rewards[-params[\"AVG_LAST\"]:]), 3)\n",
    "                ave_rewards.append(mean_reward)\n",
    "                writer.add_scalar('Reward/Episode', reward, episode)\n",
    "                writer.add_scalar('Average_Cumulative_Reward/Last_5_Episodes', mean_reward, episode)\n",
    "                print(f\"Episode {episode}, Average reward of the last {params['AVG_LAST']} episodes: {mean_reward}\")\n",
    "\n",
    "            # Update model weights if enough experiences are collected\n",
    "            if len(agent.replay_memory) >= params[\"UPDATE_RATE\"]:\n",
    "                agent.update_weights()\n",
    "\n",
    "        # Save checkpoint every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            agent.save_checkpoint(episode)\n",
    "\n",
    "    # Finalize training\n",
    "    writer.flush()\n",
    "    model_path = f'{params[\"ROOT_DIR\"]}/pongdeterministic_v4_batch{params[\"BATCH_SIZE\"]}_rate{params[\"UPDATE_RATE\"]}_ep{params[\"EPISODES\"]}.pth'\n",
    "    agent.save(model_path)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demonstrate Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_agent(params):\n",
    "    \"\"\"\n",
    "    Demonstrate the agent's performance using the trained model.\n",
    "    \n",
    "    Args:\n",
    "    params (dict): A dictionary containing demo parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Environment setup\n",
    "    env = FrameStack(gym.make('PongDeterministic-v4', render_mode='human'), num_stack=4)\n",
    "    agent = Agent(env,\n",
    "                  params[\"EPSILON_INIT\"],\n",
    "                  params[\"BATCH_SIZE\"],\n",
    "                  params[\"GAMMA\"],\n",
    "                  params[\"ROOT_DIR\"],\n",
    "                  params[\"TRAINING_MODE\"])\n",
    "    \n",
    "    # Set rendering frames per second\n",
    "    env.metadata['render_fps'] = params[\"RENDER_FPS\"]\n",
    "    \n",
    "    # Load and demonstrate the trained model\n",
    "    model_path = f'{params[\"ROOT_DIR\"]}/pongdeterministic_v4_batch{params[\"BATCH_SIZE\"]}_rate{params[\"UPDATE_RATE\"]}_ep{params[\"EPISODES\"]}.pth'\n",
    "    agent.demo(model_path)\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"BATCH_SIZE\": 8, # (8 default or 16)\n",
    "    \"UPDATE_RATE\": 10, # (3 or 10 default)\n",
    "    \"TRAINING_MODE\": None, # (True for training and False to show demonstrate agent winning pong)\n",
    "    \"EPSILON_MIN\": 0.05,\n",
    "    \"EPSILON_DECAY\": 0.995,\n",
    "    \"GAMMA\": 0.95,\n",
    "    \"EPISODES\": 1000,\n",
    "    \"EPSILON_INIT\": 1.0,\n",
    "    \"ROOT_DIR\": None,\n",
    "    \"RENDER_FPS\": 60,\n",
    "    \"AVG_LAST\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train agent with batch size with default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"BATCH_SIZE\"] = 8\n",
    "params[\"UPDATE_RATE\"] = 10\n",
    "params[\"TRAINING_MODE\"] = True\n",
    "params[\"ROOT_DIR\"] = \"model\"\n",
    "\n",
    "train_agent(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demonstrate agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"TRAINING_MODE\"] = False\n",
    "params[\"ROOT_DIR\"] = \"model\"\n",
    "\n",
    "demo_agent(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
